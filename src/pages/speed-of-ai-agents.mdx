---
slug: "/speed-of-ai-agents-versus-human/"
date: "2025-07-06"
author: "Eugene"
keywords: ai,claude code,agents,development speed
---
# How development using AI agents compares to a human developer speed

{/* cut */}
Recently I was lucky enough to run an experiment of developing a project using purely AI tools.

Instead of the team of developers it was only me, with AI tools of my choice.
At the moment experiment is 4 month old :) and continues, with pure effort spend on this project I'd say at least 3 month and 1 week.

There were complication involving me need to touch other services within the company codebase, owned by other teams, some of them cricial like payments, which took a good piece of overall timeline, but the core is a sigle service with front pages, auth hidden pages, admin, some internal APIs, workers and cron jobs.
So pretty much typical project.

At some period of time I started to doubt whether I output results the full possible speed.
A lot of people saying that they did with AI something x10 faster than it would be done without before, so reasonably this project shouldn't take 40 developer/months. I changed approaches and AI tools during this experiment, and at the moment I have a setup where I'm fully dedicated to review of that gigantic AI output and I cannot find where I can find x2..3...4..etc speedup.

And while measuring of the development speed is something that not solved well in the industry at the moment I still want to try to compare a process before heavily rely on AI agents and current in terms of how much time it saves.
Typical approaches taken in the organization ot measure velocity wouldn't work here as I do not have story points, estimations in hours, team, etc.. So I'll try to measure a one task purely relying on my understanding of my process of development with AI agents and without.
{/* cut */}

## Setup the stage

First of all it is important agree on setup and some simplification, given low predictability of the real world.
I will outline the task and describe setup of the project and development process done by human directly and the one done with ai agents.

### The project

Imagine an existing project, where developer is aware with codebase, setup and technologies used.
It is a NodeJS backend, with PostgreSQL DB and ReactJS based frontend with mobx as a state manager. Code is written on TypeScript.
There is around 30k LOC already in the project, with existing data in DB, major entities already exist.
And lets add a spice of the real world, the project depends on couple of the internal non public libraries, including components of UI design system that must be utilized.
Build, deployment automation all are known and existing, and can be just used as is.

### The task

Let's imagine that a developer needs to build a simple feature that touches all the components - public facing page where user can search the entities that lying in the one table in DB. Search by some string query, couple of filters like greater / lower prices and similar ones and with paging.

The page has reference design, but not full, missing some details, like mobile views and maybe some reaction, etc.. so it specifies the target, but still have some spaces to be figured out during development.

Lets scope this feature to a simplest implementation, no string performance measurements and UX features, even will skip for a sake of this article reflecting state in the URL.

Pretty much clear what needs to be changed / added:
- API that takes user input as request and returns paged data. Request with filters should be validated, under the hood it should go to DB with a valid and safe query to run
- Frontend including page on the some route of application, mobx store that holds a state being a mediator between rendering and backend
- Integrating all together, including due dilligence tasks like error handling and testing


### The setup

Since it is existing project lets use pretty much common setups for manual development and AI agent based development

- Tests are exist both integration and unit tests for both frontend and backend code.
- There is linting and automated formatting.
- There is ability to run the project locally with all the needed components.
- The frontend has hot reload feature, so the developer can see updates right away for most of the visual changes.

For the human development we use IDE a developer experienced and get proficcient with.
Tests and formatting are automatically run after every change the human does so we have as fast feedback during development as possible, where a developer focusing only on producing code.

For AI agent based development we are going to utilize the same setup, but with additions and modifications:

- AI agent guidelines exist and polished. For instance CLAUDE.md for claude code which is tuned for being followed well on the previous tasks
- Guidelines instruct AI agent to run formatting / lint / build / test to validate resut and that any issues must be fixed. What could be fixed automatically, i.e. formating is fixed automatically.
- Prepared allow list of the commands AI agent can run without asking permission

So basically a setup for AI agent is done in a way that a developer can give a task and expect at least a result that can be run right away.
There is no limits on AI agent tokens usage and cost. But there is a hard limits of current state of LLM context window, i.e. 200k tokens for Claude models.

### Lets do imaginary "manual" development following the developer actions

This part is becoming tricky. I have to do some approximations and assumptions here.
The final goal is to measure time spend for development, but just a pure number throwing does not look right at the moment, because it will be an expert measurement and as I told at the beginnig I started to doubt a bit in my expretness.

So lets imagine categories of tasks that a developer does with some basic time spends.
Basis is for one piece of the category, but we can apply multipliers later on.

<table>
	<thead>
	<tr>
		<th>Short Name</th>
		<th>Estimate</th>
		<th>Description</th>
		<th>Example</th>
	</tr>
	</thead>
	<tbody>
	<tr>
		<td>Thinking</td>
		<td>10m</td>
		<td>Planning the next step or initial brainstorming.</td>
		<td>Sketching a feature plan</td>
	</tr>
	<tr>
		<td>Idle</td>
		<td>5m</td>
		<td>Taking a break, such as a bio-pause or refreshing.</td>
		<td>Grabbing a coffee, restroom break</td>
	</tr>
	<tr>
		<td>Declarative Coding</td>
		<td>5m</td>
		<td>Writing code without runtime logicâ€”mainly contracts or type/interface declarations for APIs.</td>
		<td>Defining a TypeScript interface</td>
	</tr>
	<tr>
		<td>Runtime Coding</td>
		<td>15m</td>
		<td>Implementing executable code and test cases that verify the implementation works correctly.</td>
		<td>Writing a JS function with accompanying unit tests</td>
	</tr>
	<tr>
		<td>UI Coding</td>
		<td>15m</td>
		<td>Creating purely UI-related code, such as HTML/CSS, and visually confirming its correctness.</td>
		<td>Coding a button style in a React component</td>
	</tr>
	<tr>
		<td>Reading Documentation</td>
		<td>10m</td>
		<td>Reviewing relevant documentation or component/library designs to inform development.</td>
		<td>Reading API docs before using a library</td>
	</tr>
	<tr>
		<td>Refactoring</td>
		<td>10m</td>
		<td>Improving existing code structure/quality without changing its behavior.</td>
		<td>Renaming variables or reorganizing functions</td>
	</tr>
	</tbody>
</table>


Looks like we have a decent list to start from so lets move to the manual development breakdown

#### Human takes the lead

So lets imagine a human workflow

1. Thinking. On the tasks as we start, initial one could be a bit more complex - so x2 (Thinking)
2. Agreed to start from backend and do some declarative coding of the API. Request, response, some sub contracts x3 (Declarative Coding)
3. Now move to implementation - validation for API which is runtime part x1 (Runtime Coding)
4. Implementation of the filters on the existing data in DB. So we need to write some SQL / ORM queries testing it right away, given that we have couple of different filters and paging let say x3 (Runtime Coding)
5. Given that we did a bit of mess to get everything to work we spend a bit on refactoring x1 (Refactoring)
6. Huh, nice run, it works, lets do a small break x1 (Idle)
7. Now lets think on the next step x1 (Thinking)
8. Next step is defined we start to work on frontend, added a new route for the page with some boiler plate, checking that it works, which is UI coding x1 (UI Coding)
9. So we need to fetch data but before we need a shape for api contracts and state, which goes into Declarative Coding x2 (Declarative Coding)
10. Now we can draft a basic store that holds state of the filters, fetches data from api, holding that data as state and reloading it when related state is changed. It is a Runtime Coding x3 (Runtime Coding)
11. Lets bind store to some UI components and render just a data first. But we need to find a proper component and understand how to use it which reading documentation which is Reading documentation x1 and we can do UI coding x1. Boom! We have first visual results, and it even works (Reading Documentation + UI Coding)
12. Think about the UI approach for filters - what components we need, how to structure the search and filter interface x1 (Thinking)
13. Need to read documentation of the internal org UI design system library to find the right filter components and understand how to use them properly x3 (Reading Documentation)
14. Now we need to implement the actual search and filter UI components - input fields, dropdowns, etc. This requires some design decisions and component selection x3 (UI Coding)
15. We need to wire up the form inputs to the mobx store state, handling user interactions and triggering API calls on changes x2 (Runtime Coding)
16. During testing we discover some bugs in our backend filter logic - need to debug and figure out what's wrong x1 (Thinking)
17. Fix the backend issues we found during frontend testing x1 (Runtime Coding)
18. Let's add proper loading states, error handling, and empty states to make the UX smooth x3 (Runtime Coding)
19. Time to implement pagination controls and wire them to the store x1 (UI Coding)
20. Spotted a bug with paging implementation on backend during frontend testing, but it's an easy fix x1 (Runtime Coding)
21. Some point in the implementation, time to take a break after exhausting frontend work x3 (Idle)
22. Come back, thinking on what is next x1 (Thinking)
23. Looking at the reference design, we need to style everything properly and make it responsive for mobile x3 (UI Coding)
24. We notice some performance issues with rapid filter changes, so let's add debouncing to the search input x1 (Runtime Coding)
25. Quick refactoring session to clean up component structure and extract some reusable pieces x1 (Refactoring)
26. Imitating errors on UI and adding error boundaries and proper styles for error messaging x2 (UI Coding)
27. Final manual testing across different screen sizes and edge cases, fixing small bugs discovered x2 (UI Coding)
28. Code review prep - cleaning up console.logs, adding comments, ensuring code standards x1 (Refactoring)
29. Small thinking session to verify we haven't missed anything from requirements x1 (Thinking)

## Summary Table

<table>
	<thead>
	<tr>
		<th>Task Type</th>
		<th>Time per Unit</th>
		<th>Count</th>
		<th>Total Time</th>
	</tr>
	</thead>
	<tbody>
	<tr>
		<td>Thinking</td>
		<td>10m</td>
		<td>6</td>
		<td>60m</td>
	</tr>
	<tr>
		<td>Idle</td>
		<td>5m</td>
		<td>4</td>
		<td>20m</td>
	</tr>
	<tr>
		<td>Declarative Coding</td>
		<td>5m</td>
		<td>5</td>
		<td>25m</td>
	</tr>
	<tr>
		<td>Runtime Coding</td>
		<td>15m</td>
		<td>12</td>
		<td>180m</td>
	</tr>
	<tr>
		<td>UI Coding</td>
		<td>15m</td>
		<td>12</td>
		<td>180m</td>
	</tr>
	<tr>
		<td>Reading Documentation</td>
		<td>10m</td>
		<td>4</td>
		<td>40m</td>
	</tr>
	<tr>
		<td>Refactoring</td>
		<td>10m</td>
		<td>3</td>
		<td>30m</td>
	</tr>
	</tbody>
</table>

**Grand Total: 535 minutes = 8.92 hours or ~9 hours**
